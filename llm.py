import argparse
import subprocess
import os
import sys
from dotenv import load_dotenv
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold
from rich.console import Console
from rich.markdown import Markdown

load_dotenv()

SYSTEM_PROMPT_DEFAULT = "根据下面提供的上下文用中文回答问题: \n{}"
SYSTEM_PROMPT_REPO = "下面是一个代码仓库中的代码，根据代码回答我的问题: \n{}"
SYSTEM_PROMPT_TRANSCRIPT = "下面是一段转录文字，根据上下文用中文回答我的问题: \n{}"
SYSTEM_PROMPT_CUSTOM1 = (
    "根据下面提供的系列文章用中文回答问题，每篇文章的标题用=== 包围 \n{}"
)
SYSTEM_PROMPT_YOUTUBE = (
    "下面是 youtube 转录文字，第一行是视频标题，根据上下文用中文回答我的问题: \n{}"
)

SYSTEM_PROMPT = {
    "repo": SYSTEM_PROMPT_REPO,
    "default": SYSTEM_PROMPT_DEFAULT,
    "trans": SYSTEM_PROMPT_TRANSCRIPT,
}

parser = argparse.ArgumentParser(description="llm")
parser.add_argument("--prompt", default="")
parser.add_argument("--url", default="")
parser.add_argument("--youtube", default="")
parser.add_argument(
    "--sys_prompt", default="default", choices=["repo", "default", "trans"]
)
parser.add_argument("--context", required=False)
parser.add_argument("--pipe", action="store_true")
parser.add_argument("--chat", action="store_true")
parser.add_argument("--markdown", action="store_true")
args = parser.parse_args()
prompt = args.prompt


def download_subtitle(video_url):
    OUTPUT_DIR = "output"
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    sub_path = f"{OUTPUT_DIR}/%(title)s.%(ext)s"
    command = [
        "yt-dlp",
        "--write-sub",  # Write subtitle file
        "--write-auto-subs",  # Write automatic subtitle file (generated by YouTube)
        "--sub-lang",
        "en",  # Specify the language of the subtitle (e.g., English)
        "--skip-download",  # Skip downloading the video file
        "--output",
        sub_path,  # Output file format
        video_url,
    ]
    if os.getenv("PROXY"):
        command += [
            "--proxy",
            os.getenv("PROXY"),
        ]
    print(" ".join(command))
    try:
        result = subprocess.run(
            command,
            check=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
        )
        # [info] Writing video subtitles to: output\How To Reinvent Your Life In 4 Months (My Full Step-By-Step Process) ｜ Cal Newport.en.vtt
        if "Writing video subtitles to" not in result.stdout:
            print("下载字幕时出错:")
            print("stdout:", result.stdout)
            print("stderr:", result.stderr)
            return None
        print("下载完成。", result.stderr)
    except subprocess.CalledProcessError as e:
        print("下载字幕时出错:", e.stderr)
        return None
    # 这里使用一个简单的方法，获取输出目录下最新的 .srt 文件
    srt_files = [f for f in os.listdir(OUTPUT_DIR) if f.endswith(".vtt")]
    if not srt_files:
        print("未找到字幕文件。")
        return None
    # 获取最新的文件
    srt_files.sort(
        key=lambda x: os.path.getmtime(os.path.join(OUTPUT_DIR, x)), reverse=True
    )
    subtitle_path = os.path.abspath(os.path.join(OUTPUT_DIR, srt_files[0]))
    print(f"字幕文件保存为: {subtitle_path}")
    return subtitle_path


def gemini(
    q, context, use_chat=False, markdown=False, sys_prompt=SYSTEM_PROMPT_DEFAULT
):
    safety_settings = {
        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
    }
    api_key = os.getenv("GEMINI_KEY", "")
    genai.configure(api_key=api_key)
    os.environ["https_proxy"] = os.getenv("PROXY", "")
    model = genai.GenerativeModel(model_name="models/gemini-1.5-flash")
    if context:
        prompt = sys_prompt.format(context)
    else:
        prompt = f"{q}"
    if markdown:
        use_stream = False
    else:
        use_stream = True
    if use_chat:
        history = [
            {"role": "user", "parts": prompt},
            {
                "role": "model",
                "parts": "Great to meet you. What would you like to know?",
            },
        ]
        chat = model.start_chat(history=history)
        while True:
            i = input(" > ")
            if i.strip() == "":
                continue
            if i in (".exit", ".q", "q", "exit"):
                print("Exiting chat")
                break
            if i == ".md":
                print("Switching to markdown mode")
                use_stream = False
                continue
            if i == ".stream":
                print("Switching to stream mode")
                use_stream = True
                continue
            print(
                "total_tokens(chat): history=%d i=%d "
                % (
                    model.count_tokens(chat.history).total_tokens,
                    model.count_tokens(i).total_tokens,
                )
            )
            from google.generativeai.types.content_types import to_contents

            print(
                "total_tokens(chat): total=%d"
                % (model.count_tokens(chat.history + to_contents(i)).total_tokens)
            )
            response = chat.send_message(
                i, stream=use_stream, safety_settings=safety_settings
            )
            print(response.usage_metadata)
            if use_stream:
                for chunk in response:
                    print(chunk.text)
            else:
                console = Console()
                markdown = Markdown(response.text)
                console.print(markdown)
    else:
        tokens = model.count_tokens(prompt).total_tokens
        if tokens > 128 * 1000:
            print("# > 128k tokens:  $0.15  | $0.30  / 1 million tokens")
            dolor_per_token = 0.15 / 1000000
        else:
            print("# < 128k tokens:  $0.075 | $0.60  / 1 million tokens")
            dolor_per_token = 0.075 / 1000000

        print("total_tokens(prompt): ", tokens, tokens * dolor_per_token)
        response = model.generate_content(
            prompt, stream=use_stream, safety_settings=safety_settings
        )
        print(response.usage_metadata)
        if use_stream:
            for chunk in response:
                print(chunk.text)
        else:
            console = Console()
            markdown = Markdown(response.text)
            console.print(markdown)


def process_input():
    context = None
    if args.url:
        import requests

        os.environ["https_proxy"] = os.getenv("PROXY", "")
        response = requests.get("https://r.jina.ai/{}".format(args.url))
        context = response.text
    elif args.youtube:
        sub_path = download_subtitle(args.youtube)
        if not sub_path:
            raise Exception("下载字幕失败")
        with open(sub_path, encoding="utf8") as f:
            content = f.read()
            context = f"=== {sub_path} === \n\n{content}\n\n"
        args.sys_prompt = SYSTEM_PROMPT_YOUTUBE
    elif args.context:
        if os.path.isdir(args.context):
            context = ""
            for root, dirs, files in os.walk(args.context):
                for file in files:
                    fullpath = os.path.join(root, file)
                    if fullpath.endswith(".md") and "venv" not in fullpath:
                        print("add to context: ", fullpath)
                        with open(fullpath, encoding="utf8") as f:
                            content = f.read()
                            context += f"=== {fullpath} === \n\n{content}\n\n"
        else:
            with open(args.context, encoding="utf8") as f:
                context = f.read()
    elif args.pipe:
        context = sys.stdin.read()
    gemini(
        prompt,
        context,
        use_chat=args.chat,
        markdown=args.markdown,
        sys_prompt=SYSTEM_PROMPT.get(args.sys_prompt, SYSTEM_PROMPT_DEFAULT),
    )


if __name__ == "__main__":
    process_input()
